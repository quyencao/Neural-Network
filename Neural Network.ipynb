{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import scipy.io\n",
    "import sklearn\n",
    "import sklearn.datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init Parameters Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_parameters(layers_dims):\n",
    "    np.random.seed(10)\n",
    "    L = len(layers_dims)\n",
    "    parameters = {}\n",
    "    \n",
    "    for i in range(1, L):\n",
    "        parameters[\"W\" + str(i)] = np.random.randn(layers_dims[i], layers_dims[i - 1]) * np.square(2 / layers_dims[i - 1])\n",
    "        parameters[\"b\" + str(i)] = np.zeros((layers_dims[i], 1))\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameters = init_parameters([2, 4, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': array([[ 1.3315865 ,  0.71527897],\n",
       "        [-1.54540029, -0.00838385],\n",
       "        [ 0.62133597, -0.72008556],\n",
       "        [ 0.26551159,  0.10854853]]),\n",
       " 'W2': array([[ 0.00107286, -0.04365005,  0.10825655,  0.30075934]]),\n",
       " 'b1': array([[ 0.],\n",
       "        [ 0.],\n",
       "        [ 0.],\n",
       "        [ 0.]]),\n",
       " 'b2': array([[ 0.]])}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Help Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu(X):\n",
    "    return X * (X > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 0, 0])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relu(np.array([2, -2, -1, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu_backward(X):\n",
    "    return 1 * (X > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relu_backward(np.array([2,3,-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(X):\n",
    "    return 1 / (1 + np.exp(-X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.73105858,  0.88079708,  0.95257413])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(np.array([1,2,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid_backward(X):\n",
    "    return X * (1 - X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0, -2, -6])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid_backward(np.array([1,2,3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedforward Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_case_feed_forward():\n",
    "    X = np.array([\n",
    "        [1],\n",
    "        [1]\n",
    "    ])\n",
    "    parameters = init_parameters([2, 4, 1])\n",
    "    \n",
    "    Z1 = np.dot(parameters[\"W1\"], X) + parameters[\"b1\"]\n",
    "    A1 = relu(Z1)\n",
    "    Z2 = np.dot(parameters[\"W2\"], A1) + parameters[\"b2\"]\n",
    "    A2 = relu(Z2)\n",
    "    \n",
    "    return (Z1, A1, Z2, A2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Z1, A1, Z2, A2 = test_case_feed_forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(\"Z1 \\n\" + str(Z1) + \"\\n================\")\n",
    "# print(\"A1 \\n\" + str(A1) + \"\\n================\")\n",
    "# print(\"Z2 \\n\" + str(Z2) + \"\\n================\")\n",
    "# print(\"A2 \\n\" + str(A2) + \"\\n================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feed_forward(X, parameters, activation_functions = [\"relu\", \"sigmoid\"]):\n",
    "    L = len(parameters) // 2 + 1\n",
    "    caches = []\n",
    "    curr_A = X\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        A = curr_A\n",
    "        W = parameters[\"W\" + str(l)]\n",
    "        b = parameters[\"b\" + str(l)]\n",
    "        Z = np.dot(W, A) + b\n",
    "        \n",
    "        if activation_functions[l - 1] == \"sigmoid\":\n",
    "            curr_A = sigmoid(Z)\n",
    "        else:\n",
    "            curr_A = relu(Z)\n",
    "        \n",
    "        cache = (W, b, A, Z, curr_A, activation_functions[l - 1])\n",
    "        caches.append(cache)\n",
    "    return curr_A, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# AL, caches = feed_forward(np.array([[1],[1]]), parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# assert(np.array_equal(caches[0][0], parameters[\"W1\"]))\n",
    "# assert(np.array_equal(caches[0][1], parameters[\"b1\"]))\n",
    "# assert(np.array_equal(caches[0][2], np.array([[1],[1]])))\n",
    "# assert(np.array_equal(caches[0][3], Z1))\n",
    "# assert(np.array_equal(caches[0][4], A1))\n",
    "# assert(np.array_equal(caches[1][0], parameters[\"W2\"]))\n",
    "# assert(np.array_equal(caches[1][1], parameters[\"b2\"]))\n",
    "# assert(np.array_equal(caches[1][2], A1))\n",
    "# assert(np.array_equal(caches[1][3], Z2))\n",
    "# assert(np.array_equal(caches[1][4], A2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_case_back_propagation():\n",
    "    parameters = init_parameters([2, 4, 1])\n",
    "    X = np.array([\n",
    "        [1],\n",
    "        [1]\n",
    "    ])\n",
    "    Y = np.array([\n",
    "        [4]\n",
    "    ])\n",
    "    Z1, A1, Z2, A2 = test_case_feed_forward()\n",
    "    m = X.shape[1]\n",
    "    dA2 = - Y / A2 + (1 - Y) / (1 - A2)\n",
    "    dZ2 = dA2 * relu_backward(A2)\n",
    "    dW2 = 1 / m * np.dot(dZ2, A1.T)\n",
    "    db2 = 1 / m * np.sum(dZ2, axis = 1, keepdims=True)\n",
    "    dZ1 = np.dot(parameters[\"W2\"].T, dZ2) * relu_backward(A1)\n",
    "    dW1 = 1 / m * np.dot(dZ1, X.T)\n",
    "    db1 = 1 / m * np.sum(dZ1, axis = 1, keepdims=True)\n",
    "    \n",
    "    grads = {}\n",
    "    grads[\"dW1\"] = dW1\n",
    "    grads[\"dW2\"] = dW2\n",
    "    grads[\"db1\"] = db1\n",
    "    grads[\"db2\"] = db2\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# grads = test_case_back_propagation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_check(X, Y, parameters, grads, activation_functions, epsilon = 1e-7):\n",
    "    L = len(parameters) // 2 + 1\n",
    "    \n",
    "    number_parameters = get_number_parameters(parameters)\n",
    "    \n",
    "    vector = parameters_to_vector(parameters)\n",
    "    \n",
    "    \n",
    "    gradapprox = np.zeros((number_parameters, 1))\n",
    "    \n",
    "    for i in range(number_parameters):\n",
    "        vector_plus = np.copy(vector)\n",
    "        vector_plus[i][0] += epsilon\n",
    "        cost_plus = compute_cost(X, Y, vector_to_parameters(vector_plus, parameters), activation_functions)\n",
    "        \n",
    "        vector_minus = np.copy(vector)\n",
    "        vector_minus[i][0] -= epsilon\n",
    "        cost_minus = compute_cost(X, Y, vector_to_parameters(vector_minus, parameters), activation_functions)\n",
    "        \n",
    "        d = (cost_plus - cost_minus) / (2 * epsilon)\n",
    "        \n",
    "        gradapprox[i][0] = d\n",
    "    \n",
    "    grad = grads_to_vector(grads)\n",
    "    \n",
    "    numerator = np.linalg.norm(grad - gradapprox)                              \n",
    "    denominator = np.linalg.norm(grad) + np.linalg.norm(gradapprox)\n",
    "    difference = numerator / denominator\n",
    "    \n",
    "    if difference > 1e-7:\n",
    "        print (\"\\033[93m\" + \"There is a mistake in the backward propagation! difference = \" + str(difference) + \"\\033[0m\")\n",
    "    else:\n",
    "        print (\"\\033[92m\" + \"Your backward propagation works perfectly fine! difference = \" + str(difference) + \"\\033[0m\")\n",
    "    \n",
    "    return difference, vector_to_grads(gradapprox, grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_number_parameters(parameters):\n",
    "    L = len(parameters) // 2 + 1\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    for i in range(1, L):\n",
    "        W = parameters[\"W\" + str(i)]\n",
    "        b = parameters[\"b\" + str(i)]\n",
    "        \n",
    "        count += W.shape[0] * W.shape[1] + b.shape[0] * b.shape[1]\n",
    "        \n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parameters_to_vector(parameters):\n",
    "    L = len(parameters) // 2 + 1\n",
    "    \n",
    "    W = parameters[\"W1\"]\n",
    "    b = parameters[\"b1\"]\n",
    "        \n",
    "    W = W.reshape(W.shape[0] * W.shape[1], 1)\n",
    "    \n",
    "    vector = np.concatenate((W, b), axis = 0)\n",
    "    \n",
    "    for i in range(2, L):\n",
    "        W = parameters[\"W\" + str(i)]\n",
    "        b = parameters[\"b\" + str(i)]\n",
    "        \n",
    "        W = W.reshape(W.shape[0] * W.shape[1], 1)\n",
    "        \n",
    "        curr = np.concatenate((W, b), axis = 0)\n",
    "\n",
    "        vector = np.concatenate((vector, curr), axis = 0)\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# vector = parameters_to_vector(parameters)\n",
    "# assert(vector.shape[0] == 17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vector_to_parameters(vector, parameters):\n",
    "    \n",
    "    L = len(parameters) // 2 + 1\n",
    "    \n",
    "    for i in range(1, L):\n",
    "        W = parameters[\"W\" + str(i)]\n",
    "        b = parameters[\"b\" + str(i)]\n",
    "        \n",
    "        num_para_W = W.shape[0] * W.shape[1]\n",
    "        num_para_b = b.shape[0] * b.shape[1]\n",
    "        \n",
    "        vector_W = vector[:num_para_W, :]\n",
    "        vector_b = vector[num_para_W:num_para_W+num_para_b, :]\n",
    "        \n",
    "        vector = vector[num_para_W + num_para_b:, :]\n",
    "        \n",
    "        parameters[\"W\" + str(i)] = vector_W.reshape(W.shape[0], W.shape[1])\n",
    "        parameters[\"b\" + str(i)] = vector_b.reshape(b.shape[0], b.shape[1])\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# parameters = vector_to_parameters(vector, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(parameters[\"W1\"].shape == (4, 2))\n",
    "assert(parameters[\"b1\"].shape == (4, 1))\n",
    "assert(parameters[\"W2\"].shape == (1, 4))\n",
    "assert(parameters[\"b2\"].shape == (1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grads_to_vector(parameters):\n",
    "    L = len(parameters) // 2 + 1\n",
    "\n",
    "    W = parameters[\"dW1\"]\n",
    "    b = parameters[\"db1\"]\n",
    "        \n",
    "    W = W.reshape(W.shape[0] * W.shape[1], 1)\n",
    "    \n",
    "    vector = np.concatenate((W, b), axis = 0)\n",
    "    \n",
    "    for i in range(2, L):\n",
    "        W = parameters[\"dW\" + str(i)]\n",
    "        b = parameters[\"db\" + str(i)]\n",
    "        \n",
    "        W = W.reshape(W.shape[0] * W.shape[1], 1)\n",
    "        \n",
    "        curr = np.concatenate((W, b), axis = 0)\n",
    "\n",
    "        vector = np.concatenate((vector, curr), axis = 0)\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vector_to_grads(vector, grads):\n",
    "    \n",
    "    L = len(grads) // 2 + 1\n",
    "    \n",
    "    for i in range(1, L):\n",
    "        dW = grads[\"dW\" + str(i)]\n",
    "        db = grads[\"db\" + str(i)]\n",
    "        \n",
    "        num_para_dW = dW.shape[0] * dW.shape[1]\n",
    "        num_para_db = db.shape[0] * db.shape[1]\n",
    "        \n",
    "        vector_dW = vector[:num_para_dW, :]\n",
    "        vector_db = vector[num_para_dW:num_para_dW+num_para_db, :]\n",
    "        \n",
    "        vector = vector[num_para_dW + num_para_db:, :]\n",
    "        \n",
    "        grads[\"dW\" + str(i)] = vector_dW.reshape(dW.shape[0], dW.shape[1])\n",
    "        grads[\"db\" + str(i)] = vector_db.reshape(db.shape[0], db.shape[1])\n",
    "        \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_cost(X, Y, parameters, activation_functions):\n",
    "    A, caches = feed_forward(X, parameters, activation_functions)\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    cost = 1. / m * np.sum(-Y * np.log(A) - (1 - Y) * np.log(1 - A))\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# diff, gradaprox = gradient_check(np.array([[1], [1]]), np.array([[4]]), parameters, grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# gradaprox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_cross_entropy_cost(AL, Y):\n",
    "    m = AL.shape[1]\n",
    "    \n",
    "    cost = 1. / m * np.sum(-Y * np.log(AL) - (1 - Y) * np.log(1 - AL))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_mean_square_cost(AL, Y):\n",
    "    m = AL.shape[1]\n",
    "    cost = 1 / (2 * m) * np.sum(np.square(AL - Y))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def back_propagation(AL, Y, caches, loss = 'cross_entropy'):\n",
    "    m = AL.shape[1]\n",
    "    L = len(caches)\n",
    "    \n",
    "    grads = {}\n",
    "    prev_dA = {}\n",
    "    \n",
    "    W, b, prev_A, Z, curr_A, activation_func = caches[L - 1]\n",
    "    if loss == \"mean_square_error\":\n",
    "        dAL = AL - Y\n",
    "    else:\n",
    "        dAL = -Y / AL + (1 - Y) / (1 - AL)\n",
    "        \n",
    "    if activation_func == 'sigmoid':\n",
    "        dZL = dAL * sigmoid_backward(curr_A)\n",
    "    else:\n",
    "        dZL = dAL * relu_backward(curr_A)\n",
    "    \n",
    "    grads[\"dW\" + str(L)] = 1.0 / m * np.dot(dZL, prev_A.T)\n",
    "    grads[\"db\" + str(L)] = 1.0 / m * np.sum(dZL, axis = 1, keepdims=True)\n",
    "    prev_dA[\"dA\" + str(L - 1)] = np.dot(W.T, dZL)\n",
    "    \n",
    "    for l in reversed(range(1, L)):\n",
    "        W, b, prev_A, Z, curr_A, activation_func = caches[l - 1]\n",
    "        dA = prev_dA[\"dA\" + str(l)]\n",
    "        \n",
    "        if activation_func == \"sigmoid\":\n",
    "            dZ = dA * sigmoid_backward(curr_A)\n",
    "        else:\n",
    "            dZ = dA * relu_backward(curr_A)\n",
    "        \n",
    "        grads[\"dW\" + str(l)] = 1.0 / m * np.dot(dZ, prev_A.T)\n",
    "        grads[\"db\" + str(l)] = 1.0 / m * np.sum(dZ, axis = 1, keepdims=True)\n",
    "        prev_dA[\"dA\" + str(l - 1)] = np.dot(W.T, dZ)\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameters = init_parameters([2, 4, 4, 4, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "AL, caches = feed_forward(np.array([[1], [1]]), parameters, activation_functions = [\"relu\", \"sigmoid\", \"relu\", \"sigmoid\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grads = back_propagation(AL, np.array([[4]]), caches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mYour backward propagation works perfectly fine! difference = 2.86906772206e-09\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "diff, grad_aprox = gradient_check(np.array([[1], [1]]), np.array([[4]]), parameters, grads, [\"relu\", \"sigmoid\", \"relu\", \"sigmoid\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dW1': array([[ 0.02229933,  0.02229933],\n",
       "        [ 0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ],\n",
       "        [-0.0708208 , -0.0708208 ]]),\n",
       " 'dW2': array([[-0.04688101,  0.        ,  0.        , -0.0085674 ],\n",
       "        [-0.18216089,  0.        ,  0.        , -0.03328949],\n",
       "        [ 0.1770862 ,  0.        ,  0.        ,  0.03236211],\n",
       "        [-0.10530032,  0.        ,  0.        , -0.0192434 ]]),\n",
       " 'dW3': array([[-0.30453113, -0.22399288, -0.19337368, -0.17995147],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [ 0.43331269,  0.31871603,  0.27514845,  0.25605021],\n",
       "        [ 0.2257063 ,  0.16601456,  0.14332084,  0.13337284]]),\n",
       " 'dW4': array([[-1.49583702,  0.        , -0.33606177, -0.23855092]]),\n",
       " 'db1': array([[ 0.02229933],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [-0.0708208 ]]),\n",
       " 'db2': array([[-0.0229038 ],\n",
       "        [-0.08899504],\n",
       "        [ 0.08651581],\n",
       "        [-0.05144467]]),\n",
       " 'db3': array([[-0.57606185],\n",
       "        [ 0.        ],\n",
       "        [ 0.81966958],\n",
       "        [ 0.426954  ]]),\n",
       " 'db4': array([[-3.49005951]])}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_aprox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dW1': array([[ 0.02229933,  0.02229933],\n",
       "        [ 0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ],\n",
       "        [-0.0708208 , -0.0708208 ]]),\n",
       " 'dW2': array([[-0.04688101,  0.        ,  0.        , -0.0085674 ],\n",
       "        [-0.18216089,  0.        ,  0.        , -0.03328949],\n",
       "        [ 0.1770862 ,  0.        ,  0.        ,  0.03236211],\n",
       "        [-0.10530032,  0.        ,  0.        , -0.0192434 ]]),\n",
       " 'dW3': array([[-0.30453113, -0.22399288, -0.19337368, -0.17995147],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [ 0.43331269,  0.31871603,  0.27514845,  0.25605021],\n",
       "        [ 0.2257063 ,  0.16601456,  0.14332084,  0.13337284]]),\n",
       " 'dW4': array([[-1.49583702,  0.        , -0.33606177, -0.23855092]]),\n",
       " 'db1': array([[ 0.02229933],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [-0.0708208 ]]),\n",
       " 'db2': array([[-0.0229038 ],\n",
       "        [-0.08899504],\n",
       "        [ 0.08651581],\n",
       "        [-0.05144467]]),\n",
       " 'db3': array([[-0.57606185],\n",
       "        [ 0.        ],\n",
       "        [ 0.81966958],\n",
       "        [ 0.426954  ]]),\n",
       " 'db4': array([[-3.49005951]])}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Simple Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def L_model(X, Y, layers_dims, layers_activations, num_epoch = 100, learning_rate = 0.05, loss = \"cross_entropy\", print_cost = False):\n",
    "    input_dim = X.shape[0]\n",
    "    layers_dims = [input_dim] + layers_dims\n",
    "    \n",
    "    parameters = init_parameters(layers_dims)\n",
    "    \n",
    "    costs = []\n",
    "    \n",
    "    AL = None\n",
    "    \n",
    "    for epoch in range(1, num_epoch + 1):\n",
    "        \n",
    "        AL, caches = feed_forward(X, parameters, activation_functions = layers_activations)\n",
    "        \n",
    "        if loss == \"mean_square_error\":\n",
    "            cost = compute_mean_square_cost(AL, Y)\n",
    "        else:\n",
    "            cost = compute_cross_entropy_cost(AL, Y)\n",
    "        costs.append(cost)\n",
    "        \n",
    "        grads = back_propagation(AL, Y, caches, loss = loss)\n",
    "        \n",
    "        if epoch % 50 == 0:\n",
    "            diff, gradaprox = gradient_check(X, Y, parameters, grads, layers_activations)\n",
    "            # print(gradaprox)\n",
    "            # print(grads)\n",
    "        \n",
    "        parameters = update_parameters(parameters, grads, lr = learning_rate)\n",
    "        \n",
    "        if print_cost == True and epoch % 10 == 0:\n",
    "            print(\"Epoch \" + str(epoch) + \" cost: \" + str(cost))\n",
    "    \n",
    "    return AL, parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_mini_batch(X, Y, layers_dims, layers_activations, batch_size = 512, num_epoch = 100, learning_rate = 0.05, loss = \"cross_entropy\", print_cost = False):\n",
    "    input_dim = X.shape[0]\n",
    "    layers_dims = [input_dim] + layers_dims\n",
    "    \n",
    "    parameters = init_parameters(layers_dims)\n",
    "    \n",
    "    costs = []\n",
    "    \n",
    "    AL = None\n",
    "    \n",
    "    for epoch in range(1, num_epoch + 1):\n",
    "        mini_batches = random_mini_batches(X, Y, mini_batch_size = batch_size)\n",
    "        \n",
    "        for i in range(len(mini_batches)):\n",
    "            mini_batch = mini_batches[i]\n",
    "        \n",
    "            AL, caches = feed_forward(mini_batch[0], parameters, activation_functions = layers_activations)\n",
    "\n",
    "            if loss == \"mean_square_error\":\n",
    "                cost = compute_mean_square_cost(AL, mini_batch[1])\n",
    "            else:\n",
    "                cost = compute_cross_entropy_cost(AL, mini_batch[1])\n",
    "            costs.append(cost)\n",
    "\n",
    "            grads = back_propagation(AL, mini_batch[1], caches, loss = loss)\n",
    "\n",
    "            if epoch % 1000 == 0:\n",
    "                diff, gradaprox = gradient_check(mini_batch[0], mini_batch[1], parameters, grads, layers_activations)\n",
    "                \n",
    "                # print(gradaprox)\n",
    "                # print(grads)\n",
    "\n",
    "            parameters = update_parameters(parameters, grads, lr = learning_rate)\n",
    "\n",
    "        if print_cost == True and epoch % 1000 == 0:\n",
    "            print(\"Epoch \" + str(epoch) + \" cost: \" + str(cost))\n",
    "                \n",
    "    \n",
    "    return AL, parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_mini_batches(X, Y, mini_batch_size = 2, seed = 3):\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    \n",
    "    mini_batches = []\n",
    "    \n",
    "    # Shuffle\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffle_X = X[:, permutation]\n",
    "    shuffle_Y = Y[:, permutation]\n",
    "    \n",
    "    # Collect Mini Batch\n",
    "    num_complete_minibatches = math.floor(m / mini_batch_size)\n",
    "    \n",
    "    for i in range(num_complete_minibatches):\n",
    "        mini_batch_X = shuffle_X[:, i * mini_batch_size:(i + 1) * mini_batch_size]\n",
    "        mini_batch_Y = shuffle_Y[:, i * mini_batch_size:(i + 1) * mini_batch_size]\n",
    "        \n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffle_X[:, num_complete_minibatches * mini_batch_size:]\n",
    "        mini_batch_Y = shuffle_Y[:, num_complete_minibatches * mini_batch_size:]\n",
    "        \n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "        \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, lr = 0.01):\n",
    "    L = len(parameters) // 2 + 1\n",
    "    \n",
    "    for i in range(1, L):\n",
    "        parameters[\"W\" + str(i)] -= lr * grads[\"dW\" + str(i)]\n",
    "        parameters[\"b\" + str(i)] -= lr * grads[\"db\" + str(i)]\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_velocity(parameters):\n",
    "    \n",
    "    L = len(parameters) // 2\n",
    "    \n",
    "    v = {}\n",
    "    \n",
    "    for i in range(1, L + 1):\n",
    "        v[\"dW\" + str(i)] = np.zeros(parameters[\"W\" + str(i)].shape)\n",
    "        v[\"db\" + str(i)] = np.zeros(parameters[\"b\" + str(i)].shape)\n",
    "    \n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dW1': array([[ 0.,  0.],\n",
       "        [ 0.,  0.],\n",
       "        [ 0.,  0.],\n",
       "        [ 0.,  0.]]),\n",
       " 'dW2': array([[ 0.,  0.,  0.,  0.]]),\n",
       " 'db1': array([[ 0.],\n",
       "        [ 0.],\n",
       "        [ 0.],\n",
       "        [ 0.]]),\n",
       " 'db2': array([[ 0.]])}"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initialize_velocity(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_parameters_with_momentum(parameters, grads, v, beta = 0.9, lr = 0.05):\n",
    "    L = len(parameters) // 2\n",
    "    \n",
    "    for i in range(1, L + 1):\n",
    "        v[\"dW\" + str(i)] = beta * v[\"dW\" + str(i)] + (1 - beta) * grads[\"dW\" + str(i)]\n",
    "        v[\"db\" + str(i)] = beta * v[\"db\" + str(i)] + (1 - beta) * grads[\"db\" + str(i)]\n",
    "        \n",
    "        parameters[\"W\" + str(i)] -= lr *  v[\"dW\" + str(i)]\n",
    "        parameters[\"b\" + str(i)] -= lr *  v[\"db\" + str(i)]\n",
    "    \n",
    "    return parameters, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def L_model_momentum(X, Y, layers_dims, layers_activations, batch_size = 512, num_epoch = 100, learning_rate = 0.05, beta = 0, loss = \"cross_entropy\", print_cost = False):\n",
    "    input_dim = X.shape[0]\n",
    "    layers_dims = [input_dim] + layers_dims\n",
    "    \n",
    "    parameters = init_parameters(layers_dims)\n",
    "    v = initialize_velocity(parameters)\n",
    "    seed = 0\n",
    "    \n",
    "    costs = []\n",
    "    \n",
    "    AL = None\n",
    "    \n",
    "    for epoch in range(1, num_epoch + 1):\n",
    "        seed += 1\n",
    "        mini_batches = random_mini_batches(X, Y, mini_batch_size = batch_size, seed = seed)\n",
    "        \n",
    "        for mini_batch in mini_batches:\n",
    "        \n",
    "            AL, caches = feed_forward(mini_batch[0], parameters, activation_functions = layers_activations)\n",
    "\n",
    "            if loss == \"mean_square_error\":\n",
    "                cost = compute_mean_square_cost(AL, mini_batch[1])\n",
    "            else:\n",
    "                cost = compute_cross_entropy_cost(AL, mini_batch[1])\n",
    "            costs.append(cost)\n",
    "\n",
    "            grads = back_propagation(AL, mini_batch[1], caches, loss = loss)\n",
    "\n",
    "            if epoch % 1000 == 0:\n",
    "                diff, gradaprox = gradient_check(mini_batch[0], mini_batch[1], parameters, grads, layers_activations)\n",
    "                \n",
    "                # print(gradaprox)\n",
    "                # print(grads)\n",
    "\n",
    "            parameters, v = update_parameters_with_momentum(parameters, grads, v, lr = learning_rate, beta = beta)\n",
    "\n",
    "        if print_cost == True and epoch % 1000 == 0:\n",
    "            print(\"Epoch \" + str(epoch) + \" cost: \" + str(cost))\n",
    "                \n",
    "    \n",
    "    return AL, parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_adam(parameters):\n",
    "    \n",
    "    L = len(parameters) // 2\n",
    "    \n",
    "    v = {}\n",
    "    s = {}\n",
    "    \n",
    "    for i in range(1, L + 1):\n",
    "        v[\"dW\" + str(i)] = np.zeros(parameters[\"W\" + str(i)].shape)\n",
    "        v[\"db\" + str(i)] = np.zeros(parameters[\"b\" + str(i)].shape)\n",
    "        s[\"dW\" + str(i)] = np.zeros(parameters[\"W\" + str(i)].shape)\n",
    "        s[\"db\" + str(i)] = np.zeros(parameters[\"b\" + str(i)].shape)\n",
    "        \n",
    "    return s, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'dW1': array([[ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.]]),\n",
       "  'dW2': array([[ 0.,  0.,  0.,  0.]]),\n",
       "  'db1': array([[ 0.],\n",
       "         [ 0.],\n",
       "         [ 0.],\n",
       "         [ 0.]]),\n",
       "  'db2': array([[ 0.]])},\n",
       " {'dW1': array([[ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.]]),\n",
       "  'dW2': array([[ 0.,  0.,  0.,  0.]]),\n",
       "  'db1': array([[ 0.],\n",
       "         [ 0.],\n",
       "         [ 0.],\n",
       "         [ 0.]]),\n",
       "  'db2': array([[ 0.]])})"
      ]
     },
     "execution_count": 475,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initialize_adam(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def L_model_adam(X, Y, layers_dims, layers_activations, batch_size = 512, num_epoch = 100, learning_rate = 0.05, loss = \"cross_entropy\", print_cost = False):\n",
    "    input_dim = X.shape[0]\n",
    "    layers_dims = [input_dim] + layers_dims\n",
    "    \n",
    "    parameters = init_parameters(layers_dims)\n",
    "    s, v = initialize_adam(parameters)\n",
    "    seed = 0\n",
    "    t = 0\n",
    "    \n",
    "    costs = []\n",
    "    \n",
    "    AL = None\n",
    "    \n",
    "    for epoch in range(1, num_epoch + 1):\n",
    "        seed += 1\n",
    "        mini_batches = random_mini_batches(X, Y, mini_batch_size = batch_size, seed = seed)\n",
    "        \n",
    "        for mini_batch in mini_batches:\n",
    "        \n",
    "            AL, caches = feed_forward(mini_batch[0], parameters, activation_functions = layers_activations)\n",
    "\n",
    "            if loss == \"mean_square_error\":\n",
    "                cost = compute_mean_square_cost(AL, mini_batch[1])\n",
    "            else:\n",
    "                cost = compute_cross_entropy_cost(AL, mini_batch[1])\n",
    "            costs.append(cost)\n",
    "\n",
    "            grads = back_propagation(AL, mini_batch[1], caches, loss = loss)\n",
    "\n",
    "            if epoch % 1000 == 0:\n",
    "                diff, gradaprox = gradient_check(mini_batch[0], mini_batch[1], parameters, grads, layers_activations)\n",
    "                \n",
    "                # print(gradaprox)\n",
    "                # print(grads)\n",
    "            t += 1\n",
    "            parameters, s, v = update_parameters_with_adam(parameters, grads, s, v, t, lr = learning_rate)\n",
    "\n",
    "        if print_cost == True and epoch % 1000 == 0:\n",
    "            print(\"Epoch \" + str(epoch) + \" cost: \" + str(cost))\n",
    "                \n",
    "    \n",
    "    return AL, parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_parameters_with_adam(parameters, grads, s, v, t, lr = 0.1, beta1 = 0.9, beta2 = 0.999, epsilon = 1e-8):\n",
    "    \n",
    "    L = len(parameters) // 2\n",
    "    \n",
    "    v_corrected = {}\n",
    "    s_corrected = {}\n",
    "    \n",
    "    for i in range(1, L + 1):\n",
    "        v[\"dW\" + str(i)] = beta1 * v[\"dW\" + str(i)] + (1 - beta1) * grads[\"dW\" + str(i)]\n",
    "        v[\"db\" + str(i)] = beta1 * v[\"db\" + str(i)] + (1 - beta1) * grads[\"db\" + str(i)]\n",
    "        \n",
    "        v_corrected[\"dW\" + str(i)] = v[\"dW\" + str(i)] / (1 - np.power(beta1, t))\n",
    "        v_corrected[\"db\" + str(i)] = v[\"db\" + str(i)] / (1 - np.power(beta1, t))\n",
    "        \n",
    "        s[\"dW\" + str(i)] = beta2 * s[\"dW\" + str(i)] + (1 - beta2) * np.power(grads[\"dW\" + str(i)], 2)\n",
    "        s[\"db\" + str(i)] = beta2 * s[\"db\" + str(i)] + (1 - beta2) * np.power(grads[\"db\" + str(i)], 2)\n",
    "        \n",
    "        s_corrected[\"dW\" + str(i)] = s[\"dW\" + str(i)] / (1 - np.power(beta2, t))\n",
    "        s_corrected[\"db\" + str(i)] = s[\"db\" + str(i)] / (1 - np.power(beta2, t))\n",
    "        \n",
    "        parameters[\"W\" + str(i)] -= lr * v_corrected[\"dW\" + str(i)] / np.sqrt(s_corrected[\"dW\" + str(i)] + epsilon)\n",
    "        parameters[\"b\" + str(i)] -= lr * v_corrected[\"db\" + str(i)] / np.sqrt(s_corrected[\"db\" + str(i)] + epsilon)\n",
    "    \n",
    "    return parameters, s, v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(X, Y, layers_dims, layers_activations, optimizer, learning_rate = 0.0007, mini_batch_size = 64, beta = 0.9,\n",
    "          beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8, num_epochs = 10000, loss = \"cross_entropy\", print_cost = True):\n",
    "    input_dim = X.shape[0]\n",
    "    layers_dims = [input_dim] + layers_dims\n",
    "    \n",
    "    parameters = init_parameters(layers_dims)\n",
    "    \n",
    "    if optimizer == 'gd':\n",
    "        pass\n",
    "    elif optimizer == 'adam':\n",
    "        s, v = initialize_adam(parameters)\n",
    "    elif optimizer == 'momentum':\n",
    "        v = initialize_velocity(parameters)\n",
    "        \n",
    "    seed = 0\n",
    "    t = 0\n",
    "    \n",
    "    costs = []\n",
    "    \n",
    "    AL = None\n",
    "    \n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        seed += 1\n",
    "        mini_batches = random_mini_batches(X, Y, mini_batch_size = mini_batch_size, seed = seed)\n",
    "        \n",
    "        for mini_batch in mini_batches:\n",
    "            (mini_batch_X, mini_batch_Y) = mini_batch \n",
    "            AL, caches = feed_forward(mini_batch_X, parameters, activation_functions = layers_activations)\n",
    "\n",
    "            if loss == \"mean_square_error\":\n",
    "                cost = compute_mean_square_cost(AL, mini_batch_Y)\n",
    "            else:\n",
    "                cost = compute_cross_entropy_cost(AL, mini_batch_Y)\n",
    "            costs.append(cost)\n",
    "\n",
    "            grads = back_propagation(AL, mini_batch_Y, caches, loss = loss)\n",
    "\n",
    "            if epoch % 1000 == 0:\n",
    "                diff, gradaprox = gradient_check(mini_batch_X, mini_batch_Y, parameters, grads, layers_activations)\n",
    "                \n",
    "                # print(gradaprox)\n",
    "                # print(grads)\n",
    "            if optimizer == 'gd':\n",
    "                parameters = update_parameters(parameters, grads, lr = learning_rate)\n",
    "            elif optimizer == 'adam':\n",
    "                t += 1\n",
    "                parameters, s, v = update_parameters_with_adam(parameters, grads, s, v, t, lr = learning_rate, beta1 = beta1, beta2 = beta2)\n",
    "            elif optimizer == 'momentum':\n",
    "                parameters, v = update_parameters_with_momentum(parameters, grads, v, lr = learning_rate, beta = beta)\n",
    "\n",
    "        if print_cost == True and epoch % 1000 == 0:\n",
    "            print(\"Epoch \" + str(epoch) + \" cost: \" + str(cost))\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mYour backward propagation works perfectly fine! difference = 3.55532248895e-09\u001b[0m\n",
      "\u001b[92mYour backward propagation works perfectly fine! difference = 2.48251672928e-09\u001b[0m\n",
      "Epoch 1000 cost: 0.27007980868\n"
     ]
    }
   ],
   "source": [
    "X = np.array([\n",
    "    [1, 0, 0, 1],\n",
    "    [0, 1, 0, 1]\n",
    "])\n",
    "\n",
    "Y = np.array([\n",
    "    [0, 1, 1, 0]\n",
    "])\n",
    "AL, parameters = L_model_momentum(X, Y, [4, 4, 1], [\"sigmoid\", \"sigmoid\", \"sigmoid\"], num_epoch = 1000, batch_size = 2, learning_rate = 0.05, beta = 0.1, print_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "AL, caches = feed_forward(X, parameters, activation_functions = [\"sigmoid\", \"sigmoid\", \"sigmoid\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    np.random.seed(3)\n",
    "    train_X, train_Y = sklearn.datasets.make_moons(n_samples=300, noise=.2) #300 #0.2 \n",
    "    # Visualize the data\n",
    "    plt.scatter(train_X[:, 0], train_X[:, 1], c=train_Y, s=40, cmap=plt.cm.Spectral);\n",
    "    train_X = train_X.T\n",
    "    train_Y = train_Y.reshape((1, train_Y.shape[0]))\n",
    "    \n",
    "    return train_X, train_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X, train_Y = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 300)"
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 446,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AL, parameters = L_model(train_X, train_Y, [4, 1], [\"relu\", \"sigmoid\"], num_epoch = 9000, learning_rate = 0.0007, print_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AL, parameters = L_model_mini_batch(train_X, train_Y, [4, 1], [\"relu\", \"sigmoid\"], batch_size = 64, num_epoch = 5000, learning_rate = 0.0007, print_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AL, parameters = L_model_momentum(train_X, train_Y, [4, 1], [\"relu\", \"sigmoid\"], batch_size = 64, num_epoch = 5000, beta = 0.0007, learning_rate = 1.3, print_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AL, parameters = L_model_adam(train_X, train_Y, [4, 1], [\"relu\", \"sigmoid\"], batch_size = 64, num_epoch = 5000, learning_rate = 0.0007, print_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.863333333333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1,\n",
       "        1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,\n",
       "        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,\n",
       "        1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,\n",
       "        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,\n",
       "        0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,\n",
       "        0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,\n",
       "        1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,\n",
       "        1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
       "        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "        1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,\n",
       "        0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n",
       "        1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0]])"
      ]
     },
     "execution_count": 549,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(train_X, train_Y, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, y, parameters):\n",
    "    \"\"\"\n",
    "    This function is used to predict the results of a  n-layer neural network.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data set of examples you would like to label\n",
    "    parameters -- parameters of the trained model\n",
    "    \n",
    "    Returns:\n",
    "    p -- predictions for the given dataset X\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    p = np.zeros((1,m), dtype = np.int)\n",
    "    \n",
    "    # Forward propagation\n",
    "    a3, caches = feed_forward(X, parameters)\n",
    "    \n",
    "    # convert probas to 0/1 predictions\n",
    "    for i in range(0, a3.shape[1]):\n",
    "        if a3[0,i] > 0.5:\n",
    "            p[0,i] = 1\n",
    "        else:\n",
    "            p[0,i] = 0\n",
    "\n",
    "    # print results\n",
    "\n",
    "    #print (\"predictions: \" + str(p[0,:]))\n",
    "    #print (\"true labels: \" + str(y[0,:]))\n",
    "    print(\"Accuracy: \"  + str(np.mean((p[0,:] == y[0,:]))))\n",
    "    \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mYour backward propagation works perfectly fine! difference = 2.79096398114e-09\u001b[0m\n",
      "\u001b[92mYour backward propagation works perfectly fine! difference = 4.08622796797e-09\u001b[0m\n",
      "\u001b[92mYour backward propagation works perfectly fine! difference = 3.8423809087e-09\u001b[0m\n",
      "\u001b[92mYour backward propagation works perfectly fine! difference = 4.40647394978e-09\u001b[0m\n",
      "\u001b[92mYour backward propagation works perfectly fine! difference = 3.57256786281e-09\u001b[0m\n",
      "Epoch 1000 cost: 0.538993036609\n",
      "\u001b[92mYour backward propagation works perfectly fine! difference = 2.12997664622e-09\u001b[0m\n",
      "\u001b[92mYour backward propagation works perfectly fine! difference = 4.93531755191e-09\u001b[0m\n",
      "\u001b[92mYour backward propagation works perfectly fine! difference = 2.00534668895e-09\u001b[0m\n",
      "\u001b[92mYour backward propagation works perfectly fine! difference = 2.46462078201e-09\u001b[0m\n",
      "\u001b[92mYour backward propagation works perfectly fine! difference = 5.38237644194e-09\u001b[0m\n",
      "Epoch 2000 cost: 0.458676724502\n",
      "\u001b[92mYour backward propagation works perfectly fine! difference = 1.63145739633e-09\u001b[0m\n",
      "\u001b[92mYour backward propagation works perfectly fine! difference = 3.26294384347e-09\u001b[0m\n",
      "\u001b[92mYour backward propagation works perfectly fine! difference = 2.09955320301e-09\u001b[0m\n",
      "\u001b[92mYour backward propagation works perfectly fine! difference = 3.78734758586e-09\u001b[0m\n",
      "\u001b[92mYour backward propagation works perfectly fine! difference = 2.40312658824e-09\u001b[0m\n",
      "Epoch 3000 cost: 0.376108864412\n",
      "\u001b[92mYour backward propagation works perfectly fine! difference = 2.73310132823e-09\u001b[0m\n",
      "\u001b[92mYour backward propagation works perfectly fine! difference = 3.65422004824e-09\u001b[0m\n",
      "\u001b[92mYour backward propagation works perfectly fine! difference = 3.45460240642e-09\u001b[0m\n",
      "\u001b[92mYour backward propagation works perfectly fine! difference = 3.12599112492e-09\u001b[0m\n",
      "\u001b[92mYour backward propagation works perfectly fine! difference = 5.48742675691e-09\u001b[0m\n",
      "Epoch 4000 cost: 0.422588461668\n",
      "\u001b[92mYour backward propagation works perfectly fine! difference = 2.29971460772e-09\u001b[0m\n",
      "\u001b[92mYour backward propagation works perfectly fine! difference = 4.44312332489e-09\u001b[0m\n",
      "\u001b[92mYour backward propagation works perfectly fine! difference = 7.21492358367e-09\u001b[0m\n",
      "\u001b[92mYour backward propagation works perfectly fine! difference = 3.42615412034e-09\u001b[0m\n",
      "\u001b[92mYour backward propagation works perfectly fine! difference = 5.78740939063e-09\u001b[0m\n",
      "Epoch 5000 cost: 0.346836384316\n",
      "\u001b[92mYour backward propagation works perfectly fine! difference = 4.18040739548e-09\u001b[0m\n",
      "\u001b[92mYour backward propagation works perfectly fine! difference = 3.30884010608e-09\u001b[0m\n",
      "\u001b[92mYour backward propagation works perfectly fine! difference = 5.44561546425e-09\u001b[0m\n",
      "\u001b[92mYour backward propagation works perfectly fine! difference = 2.65024238477e-09\u001b[0m\n",
      "\u001b[92mYour backward propagation works perfectly fine! difference = 5.93663892197e-09\u001b[0m\n",
      "Epoch 6000 cost: 0.342733222885\n",
      "\u001b[92mYour backward propagation works perfectly fine! difference = 4.73264589281e-09\u001b[0m\n",
      "\u001b[92mYour backward propagation works perfectly fine! difference = 2.94808861452e-09\u001b[0m\n",
      "\u001b[92mYour backward propagation works perfectly fine! difference = 3.62566642094e-09\u001b[0m\n",
      "\u001b[92mYour backward propagation works perfectly fine! difference = 6.97054257653e-09\u001b[0m\n",
      "\u001b[92mYour backward propagation works perfectly fine! difference = 3.92087960923e-09\u001b[0m\n",
      "Epoch 7000 cost: 0.331366475508\n",
      "\u001b[92mYour backward propagation works perfectly fine! difference = 5.56382038684e-09\u001b[0m\n",
      "\u001b[92mYour backward propagation works perfectly fine! difference = 6.32016767383e-09\u001b[0m\n",
      "\u001b[92mYour backward propagation works perfectly fine! difference = 3.9518534236e-09\u001b[0m\n",
      "\u001b[92mYour backward propagation works perfectly fine! difference = 5.90624057657e-09\u001b[0m\n",
      "\u001b[92mYour backward propagation works perfectly fine! difference = 1.72601923416e-08\u001b[0m\n",
      "Epoch 8000 cost: 0.43705570485\n",
      "\u001b[92mYour backward propagation works perfectly fine! difference = 2.30236872363e-09\u001b[0m\n",
      "\u001b[92mYour backward propagation works perfectly fine! difference = 1.84360356073e-09\u001b[0m\n",
      "\u001b[92mYour backward propagation works perfectly fine! difference = 1.25841993357e-08\u001b[0m\n",
      "\u001b[92mYour backward propagation works perfectly fine! difference = 3.03550912793e-09\u001b[0m\n",
      "\u001b[92mYour backward propagation works perfectly fine! difference = 1.21877181016e-08\u001b[0m\n",
      "Epoch 9000 cost: 0.308701596282\n",
      "\u001b[92mYour backward propagation works perfectly fine! difference = 3.88335927799e-09\u001b[0m\n",
      "\u001b[92mYour backward propagation works perfectly fine! difference = 3.87474358528e-09\u001b[0m\n",
      "\u001b[92mYour backward propagation works perfectly fine! difference = 1.11397857277e-08\u001b[0m\n",
      "\u001b[92mYour backward propagation works perfectly fine! difference = 6.72370269606e-09\u001b[0m\n",
      "\u001b[92mYour backward propagation works perfectly fine! difference = 4.60310850399e-09\u001b[0m\n",
      "Epoch 10000 cost: 0.233361596636\n"
     ]
    }
   ],
   "source": [
    "parameters = model(train_X, train_Y, [4, 1], [\"relu\", \"sigmoid\"], \"momentum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
